{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61dabcad",
   "metadata": {},
   "source": [
    "# TP Classification de Tumeurs C√©r√©brales avec CNN\n",
    "\n",
    "## Objectifs du TP\n",
    "1. Concevoir un r√©seau de neurones convolutionnel (CNN) pour classer les images d'IRM du cerveau selon la pr√©sence et le type de tumeur\n",
    "2. Param√©trer un CNN (nombre de filtres, taille des noyaux, Dropout, batch size, etc.)\n",
    "3. Exp√©rimenter plusieurs combinaisons de param√®tres et observer leur impact sur la performance\n",
    "4. Comparer votre mod√®le avec des mod√®les pr√©-entra√Æn√©s (Transfer Learning)\n",
    "\n",
    "## Dataset : Brain Tumor MRI\n",
    "- **4 classes** : glioma_tumor, meningioma_tumor, pituitary_tumor, no_tumor\n",
    "- **Source** : Kaggle Brain Tumor Classification MRI Dataset\n",
    "- **Images** : IRM c√©r√©brales annot√©es\n",
    "\n",
    "## Contexte M√©dical\n",
    "Les tumeurs c√©r√©brales sont des masses anormales de cellules dans le cerveau. L'analyse des images IRM permet de d√©tecter ces tumeurs pr√©cocement. Les techniques d'intelligence artificielle peuvent aider √† automatiser cette d√©tection et am√©liorer la pr√©cision du diagnostic.\n",
    "\n",
    "---\n",
    "**Note** : Ce notebook est con√ßu pour Google Colab avec GPU activ√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f34b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPORTS ET CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# TensorFlow et Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Flatten, Conv2D, MaxPooling2D, Dropout, \n",
    "    BatchNormalization, GlobalAveragePooling2D\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications import ResNet50, VGG16, DenseNet121\n",
    "\n",
    "# Metrics et √©valuation\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    precision_score, recall_score, f1_score, accuracy_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Configuration matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Afficher la version de TensorFlow\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "\n",
    "# V√©rifier GPU disponible\n",
    "print(f\"\\nGPU disponible: {tf.config.list_physical_devices('GPU')}\")\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"‚úì GPU d√©tect√© - Acc√©l√©ration activ√©e\")\n",
    "else:\n",
    "    print(\"‚ö† Pas de GPU - Entra√Ænement sur CPU (plus lent)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b2575d",
   "metadata": {},
   "source": [
    "## 1. Chargement et Exploration du Dataset\n",
    "\n",
    "### Montage Google Drive (pour Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4621c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MONTAGE GOOGLE DRIVE ET CHEMINS\n",
    "# ============================================\n",
    "\n",
    "# Montage Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Chemins vers le dataset sur Google Drive\n",
    "# Le dossier IRM contient Training et Testing avec les 4 classes de tumeurs\n",
    "TRAIN_DIR = '/content/drive/MyDrive/IRM/Training'\n",
    "TEST_DIR = '/content/drive/MyDrive/IRM/Testing'\n",
    "\n",
    "print(\"‚úì Google Drive mont√© avec succ√®s\")\n",
    "print(f\"Train path: {TRAIN_DIR}\")\n",
    "print(f\"Test path: {TEST_DIR}\")\n",
    "\n",
    "# V√©rifier que les dossiers existent\n",
    "if os.path.exists(TRAIN_DIR):\n",
    "    print(f\"\\n‚úì Training directory trouv√©\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå ERREUR: Training directory non trouv√©: {TRAIN_DIR}\")\n",
    "    \n",
    "if os.path.exists(TEST_DIR):\n",
    "    print(f\"‚úì Testing directory trouv√©\")\n",
    "else:\n",
    "    print(f\"‚ùå ERREUR: Testing directory non trouv√©: {TEST_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362515e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXPLORATION DU DATASET\n",
    "# ============================================\n",
    "\n",
    "# Liste des classes\n",
    "if os.path.exists(TRAIN_DIR):\n",
    "    class_names = sorted(os.listdir(TRAIN_DIR))\n",
    "    print(f\"Classes d√©tect√©es: {class_names}\")\n",
    "    print(f\"Nombre de classes: {len(class_names)}\")\n",
    "    \n",
    "    # Compter le nombre d'images par classe\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DISTRIBUTION DES IMAGES - TRAINING SET\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    train_counts = {}\n",
    "    total_train = 0\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(TRAIN_DIR, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            count = len([f for f in os.listdir(class_path) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "            train_counts[class_name] = count\n",
    "            total_train += count\n",
    "            percentage = (count / 2870) * 100  # Total approximatif\n",
    "            print(f\"{class_name:20s}: {count:4d} images ({percentage:5.1f}%)\")\n",
    "    \n",
    "    print(f\"{'TOTAL':20s}: {total_train:4d} images\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DISTRIBUTION DES IMAGES - TESTING SET\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    test_counts = {}\n",
    "    total_test = 0\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(TEST_DIR, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            count = len([f for f in os.listdir(class_path) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "            test_counts[class_name] = count\n",
    "            total_test += count\n",
    "            percentage = (count / 394) * 100  # Total approximatif\n",
    "            print(f\"{class_name:20s}: {count:4d} images ({percentage:5.1f}%)\")\n",
    "    \n",
    "    print(f\"{'TOTAL':20s}: {total_test:4d} images\")\n",
    "    \n",
    "    # D√©tecter d√©s√©quilibre\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ANALYSE DU D√âS√âQUILIBRE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    min_count = min(train_counts.values())\n",
    "    max_count = max(train_counts.values())\n",
    "    ratio = max_count / min_count\n",
    "    \n",
    "    print(f\"Ratio max/min: {ratio:.2f}\")\n",
    "    if ratio > 2.0:\n",
    "        print(\"‚ö† D√âS√âQUILIBRE SIGNIFICATIF D√âTECT√â!\")\n",
    "        print(\"   ‚Üí Utilisation de class_weight recommand√©e\")\n",
    "        print(\"   ‚Üí Data augmentation n√©cessaire pour la classe minoritaire\")\n",
    "    else:\n",
    "        print(\"‚úì Distribution relativement √©quilibr√©e\")\n",
    "    \n",
    "    # Visualiser la distribution\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Training set\n",
    "    ax1.bar(train_counts.keys(), train_counts.values(), color='steelblue', alpha=0.8)\n",
    "    ax1.set_title('Distribution Training Set', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Classes')\n",
    "    ax1.set_ylabel('Nombre d\\'images')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    for i, (k, v) in enumerate(train_counts.items()):\n",
    "        ax1.text(i, v + 20, str(v), ha='center', fontweight='bold')\n",
    "    \n",
    "    # Testing set\n",
    "    ax2.bar(test_counts.keys(), test_counts.values(), color='coral', alpha=0.8)\n",
    "    ax2.set_title('Distribution Testing Set', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Classes')\n",
    "    ax2.set_ylabel('Nombre d\\'images')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    for i, (k, v) in enumerate(test_counts.items()):\n",
    "        ax2.text(i, v + 3, str(v), ha='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ùå Impossible d'explorer le dataset - Dossier Training introuvable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a150129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALISATION D'EXEMPLES D'IMAGES\n",
    "# ============================================\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "def display_sample_images(data_dir, class_names, samples_per_class=3):\n",
    "    \"\"\"Affiche des exemples d'images pour chaque classe\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(len(class_names), samples_per_class, \n",
    "                             figsize=(15, 4*len(class_names)))\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        images_list = [f for f in os.listdir(class_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        # S√©lectionner quelques images au hasard\n",
    "        selected_images = np.random.choice(images_list, \n",
    "                                          size=min(samples_per_class, len(images_list)), \n",
    "                                          replace=False)\n",
    "        \n",
    "        for j, img_name in enumerate(selected_images):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            img = image.load_img(img_path, target_size=(150, 150))\n",
    "            img_array = image.img_to_array(img) / 255.0\n",
    "            \n",
    "            if len(class_names) == 1:\n",
    "                ax = axes[j]\n",
    "            else:\n",
    "                ax = axes[i, j]\n",
    "            \n",
    "            ax.imshow(img_array)\n",
    "            ax.axis('off')\n",
    "            \n",
    "            if j == 0:\n",
    "                ax.set_title(f'{class_name}\\n{img_name}', \n",
    "                           fontsize=10, fontweight='bold', loc='left')\n",
    "            else:\n",
    "                ax.set_title(img_name, fontsize=9)\n",
    "    \n",
    "    plt.suptitle('Exemples d\\'Images IRM par Classe', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Afficher les exemples\n",
    "if os.path.exists(TRAIN_DIR):\n",
    "    display_sample_images(TRAIN_DIR, class_names, samples_per_class=4)\n",
    "else:\n",
    "    print(\"‚ùå Impossible d'afficher les exemples - Dossier Training introuvable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52abb2f5",
   "metadata": {},
   "source": [
    "## 2. Pr√©paration des Donn√©es\n",
    "\n",
    "### Configuration des param√®tres et ImageDataGenerator avec Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4fc7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION DES HYPERPARAM√àTRES\n",
    "# ============================================\n",
    "\n",
    "# Param√®tres d'images\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "IMG_CHANNELS = 3\n",
    "IMG_SIZE = (IMG_HEIGHT, IMG_WIDTH)\n",
    "\n",
    "# Param√®tres d'entra√Ænement (modifiables pour exp√©rimentation)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# Nombre de classes\n",
    "NUM_CLASSES = 4  # glioma, meningioma, pituitary, no_tumor\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Taille des images: {IMG_WIDTH}x{IMG_HEIGHT}x{IMG_CHANNELS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Nombre d'√©poques: {EPOCHS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Nombre de classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe30cf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DATA AUGMENTATION - IMAGEDATAGENERATOR\n",
    "# ============================================\n",
    "\n",
    "# Generator pour le training avec augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,              # Normalisation des pixels [0, 1]\n",
    "    rotation_range=20,           # Rotation al√©atoire ¬±20 degr√©s\n",
    "    width_shift_range=0.2,       # D√©calage horizontal ¬±20%\n",
    "    height_shift_range=0.2,      # D√©calage vertical ¬±20%\n",
    "    shear_range=0.15,            # Cisaillement\n",
    "    zoom_range=0.2,              # Zoom al√©atoire ¬±20%\n",
    "    horizontal_flip=True,        # Retournement horizontal\n",
    "    fill_mode='nearest',         # Remplissage des pixels manquants\n",
    "    brightness_range=[0.8, 1.2]  # Variation de luminosit√©\n",
    ")\n",
    "\n",
    "# Generator pour la validation/test - SEULEMENT normalisation\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "print(\"‚úì ImageDataGenerators cr√©√©s\")\n",
    "print(\"\\nAugmentations appliqu√©es au training:\")\n",
    "print(\"  - Rotation: ¬±20¬∞\")\n",
    "print(\"  - D√©calage: ¬±20%\")\n",
    "print(\"  - Cisaillement: 0.15\")\n",
    "print(\"  - Zoom: ¬±20%\")\n",
    "print(\"  - Flip horizontal: Oui\")\n",
    "print(\"  - Luminosit√©: 0.8-1.2\")\n",
    "print(\"\\n‚úì Test set: Normalisation uniquement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5482d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CR√âATION DES G√âN√âRATEURS DE DONN√âES\n",
    "# ============================================\n",
    "\n",
    "# Training generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',  # Multi-classes (4 classes)\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Test generator\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Important pour l'√©valuation\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"G√âN√âRATEURS CR√â√âS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nTraining samples: {train_generator.samples}\")\n",
    "print(f\"Test samples: {test_generator.samples}\")\n",
    "print(f\"\\nClasses d√©tect√©es: {list(train_generator.class_indices.keys())}\")\n",
    "print(f\"Indices des classes: {train_generator.class_indices}\")\n",
    "print(f\"\\nBatch size: {BATCH_SIZE}\")\n",
    "print(f\"Steps per epoch: {train_generator.samples // BATCH_SIZE}\")\n",
    "print(f\"Validation steps: {test_generator.samples // BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba56ec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CALCUL DES CLASS WEIGHTS (gestion du d√©s√©quilibre)\n",
    "# ============================================\n",
    "\n",
    "# Extraire les labels du training generator\n",
    "train_labels = train_generator.classes\n",
    "\n",
    "# Calculer les poids des classes\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "\n",
    "# Convertir en dictionnaire\n",
    "class_weights = {i: weight for i, weight in enumerate(class_weights_array)}\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"CLASS WEIGHTS (pour g√©rer le d√©s√©quilibre)\")\n",
    "print(\"=\"*50)\n",
    "for class_name, class_idx in train_generator.class_indices.items():\n",
    "    print(f\"{class_name:20s} (index {class_idx}): poids = {class_weights[class_idx]:.3f}\")\n",
    "\n",
    "print(\"\\n‚úì Les class weights seront utilis√©s lors de l'entra√Ænement\")\n",
    "print(\"  ‚Üí Les classes minoritaires auront plus de poids dans la loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ecf3b",
   "metadata": {},
   "source": [
    "## 3. Construction du Mod√®le CNN - Fonction Param√©trable\n",
    "\n",
    "### Fonction Factory pour g√©n√©rer diff√©rentes architectures CNN\n",
    "\n",
    "Cette fonction permet de tester facilement diff√©rentes configurations:\n",
    "- **Nombre de filtres** : [32,64,128] ou [16,32,64,128] ou [32,64,128,256]\n",
    "- **Taille du kernel** : 3x3 ou 5x5\n",
    "- **Taux de Dropout** : 0.3, 0.4, 0.5\n",
    "- **BatchNormalization** : pour acc√©l√©rer l'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdbbec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ARCHITECTURE CNN OPTIMIS√âE POUR DE MEILLEURS R√âSULTATS\n",
    "# ============================================\n",
    "\n",
    "def build_advanced_cnn_model(filters_list=[64, 128, 256, 512],\n",
    "                             kernel_size=3,\n",
    "                             dropout_rate=0.4,\n",
    "                             use_batch_norm=True,\n",
    "                             dense_units=[512, 256],\n",
    "                             input_shape=(224, 224, 3),\n",
    "                             num_classes=4):\n",
    "    \"\"\"\n",
    "    Construit un mod√®le CNN avanc√© optimis√© pour la classification d'images m√©dicales.\n",
    "\n",
    "    Am√©liorations par rapport √† la version basique :\n",
    "    - Blocs convolutionnels multiples par √©tage (inspiration VGG)\n",
    "    - Global Average Pooling au lieu de Flatten pour moins de param√®tres\n",
    "    - Architecture en pyramide avec augmentation progressive des filtres\n",
    "    - R√©gularisation am√©lior√©e avec Dropout spatial et L2\n",
    "    - Couches denses multiples avec r√©gularisation\n",
    "\n",
    "    Param√®tres:\n",
    "    -----------\n",
    "    filters_list : list\n",
    "        Liste du nombre de filtres pour chaque bloc (ex: [64, 128, 256, 512])\n",
    "    kernel_size : int\n",
    "        Taille du kernel de convolution (3 recommand√©)\n",
    "    dropout_rate : float\n",
    "        Taux de dropout (0.3-0.5)\n",
    "    use_batch_norm : bool\n",
    "        Utiliser BatchNormalization\n",
    "    dense_units : list\n",
    "        Nombre de neurones dans les couches denses [dense1, dense2]\n",
    "    input_shape : tuple\n",
    "        Dimensions de l'image d'entr√©e\n",
    "    num_classes : int\n",
    "        Nombre de classes\n",
    "\n",
    "    Retourne:\n",
    "    ---------\n",
    "    model : Sequential\n",
    "        Mod√®le Keras optimis√©\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential(name=f'Advanced_CNN_{len(filters_list)}blocks')\n",
    "\n",
    "    # Bloc d'entr√©e avec r√©gularisation\n",
    "    model.add(Conv2D(filters_list[0], (kernel_size, kernel_size),\n",
    "                     activation='relu',\n",
    "                     padding='same',\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                     input_shape=input_shape,\n",
    "                     name=f'conv_block1_1'))\n",
    "    model.add(Conv2D(filters_list[0], (kernel_size, kernel_size),\n",
    "                     activation='relu',\n",
    "                     padding='same',\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                     name=f'conv_block1_2'))\n",
    "    if use_batch_norm:\n",
    "        model.add(BatchNormalization(name='bn_block1'))\n",
    "    model.add(MaxPooling2D((2, 2), name='pool_block1'))\n",
    "    model.add(Dropout(0.2, name='dropout_block1'))  # Dropout spatial l√©ger\n",
    "\n",
    "    # Blocs suivants avec augmentation des filtres\n",
    "    for i, filters in enumerate(filters_list[1:], start=2):\n",
    "        # Premi√®re conv du bloc\n",
    "        model.add(Conv2D(filters, (kernel_size, kernel_size),\n",
    "                        activation='relu',\n",
    "                        padding='same',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                        name=f'conv_block{i}_1'))\n",
    "\n",
    "        # Deuxi√®me conv du bloc (inspiration VGG)\n",
    "        model.add(Conv2D(filters, (kernel_size, kernel_size),\n",
    "                        activation='relu',\n",
    "                        padding='same',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                        name=f'conv_block{i}_2'))\n",
    "\n",
    "        if use_batch_norm:\n",
    "            model.add(BatchNormalization(name=f'bn_block{i}'))\n",
    "\n",
    "        model.add(MaxPooling2D((2, 2), name=f'pool_block{i}'))\n",
    "        model.add(Dropout(dropout_rate, name=f'dropout_block{i}'))\n",
    "\n",
    "    # Remplacement de Flatten par Global Average Pooling (moins de param√®tres, meilleure g√©n√©ralisation)\n",
    "    model.add(GlobalAveragePooling2D(name='global_avg_pool'))\n",
    "\n",
    "    # Couches denses avec r√©gularisation progressive\n",
    "    for j, units in enumerate(dense_units):\n",
    "        model.add(Dense(units,\n",
    "                       activation='relu',\n",
    "                       kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                       name=f'dense_{j+1}_{units}'))\n",
    "        if use_batch_norm:\n",
    "            model.add(BatchNormalization(name=f'bn_dense_{j+1}'))\n",
    "        model.add(Dropout(dropout_rate + 0.1, name=f'dropout_dense_{j+1}'))  # Dropout plus √©lev√©\n",
    "\n",
    "    # Couche de sortie\n",
    "    model.add(Dense(num_classes,\n",
    "                   activation='softmax',\n",
    "                   kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                   name='output'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Fonction pour compiler avec optimiseur optimis√©\n",
    "def compile_model_advanced(model, learning_rate=1e-3):\n",
    "    \"\"\"\n",
    "    Compile le mod√®le avec un optimiseur AdamW et callbacks optimis√©s\n",
    "    \"\"\"\n",
    "    optimizer = tf.keras.optimizers.AdamW(\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=1e-4,  # L2 regularization via weight decay\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-7\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy',\n",
    "                tf.keras.metrics.Precision(name='precision'),\n",
    "                tf.keras.metrics.Recall(name='recall'),\n",
    "                tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Fonction pour cr√©er les callbacks optimis√©s\n",
    "def get_callbacks_advanced(model_name=\"model\"):\n",
    "    \"\"\"\n",
    "    Retourne une liste de callbacks optimis√©s pour l'entra√Ænement\n",
    "    \"\"\"\n",
    "    callbacks = [\n",
    "        # Arr√™t pr√©coce avec patience r√©duite pour √©viter l'overfitting\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=15,  # Plus de patience pour convergence\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "\n",
    "        # R√©duction du learning rate sur plateau\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,  # R√©duction plus agressive\n",
    "            patience=7,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "\n",
    "        # Sauvegarde du meilleur mod√®le\n",
    "        ModelCheckpoint(\n",
    "            f'best_{model_name}.keras',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "\n",
    "        # Learning rate scheduler personnalis√© (optionnel)\n",
    "        tf.keras.callbacks.LearningRateScheduler(\n",
    "            lambda epoch: 1e-3 * (0.95 ** epoch),  # D√©croissance exponentielle l√©g√®re\n",
    "            verbose=0\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return callbacks\n",
    "\n",
    "\n",
    "print(\"‚úì Fonction build_advanced_cnn_model() d√©finie avec architecture optimis√©e\")\n",
    "print(\"‚úì Fonction compile_model_advanced() pour compilation optimis√©e\")\n",
    "print(\"‚úì Fonction get_callbacks_advanced() pour callbacks avanc√©s\")\n",
    "print(\"‚úì Pr√™t pour des performances sup√©rieures !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbc6151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FONCTION D'ENTRA√éNEMENT ET VISUALISATION\n",
    "# ============================================\n",
    "\n",
    "def train_and_evaluate(model, train_gen, test_gen, model_name, \n",
    "                       epochs=15, batch_size=32, class_weights=None, \n",
    "                       use_callbacks=True):\n",
    "    \"\"\"\n",
    "    Entra√Æne un mod√®le et affiche les courbes d'apprentissage.\n",
    "    \n",
    "    Param√®tres:\n",
    "    -----------\n",
    "    model : Sequential\n",
    "        Mod√®le Keras √† entra√Æner\n",
    "    train_gen : ImageDataGenerator\n",
    "        G√©n√©rateur de donn√©es d'entra√Ænement\n",
    "    test_gen : ImageDataGenerator\n",
    "        G√©n√©rateur de donn√©es de validation\n",
    "    model_name : str\n",
    "        Nom du mod√®le pour les graphiques\n",
    "    epochs : int\n",
    "        Nombre d'√©poques\n",
    "    batch_size : int\n",
    "        Taille du batch\n",
    "    class_weights : dict\n",
    "        Poids des classes\n",
    "    use_callbacks : bool\n",
    "        Utiliser les callbacks (EarlyStopping, ReduceLROnPlateau)\n",
    "    \n",
    "    Retourne:\n",
    "    ---------\n",
    "    history : History\n",
    "        Historique de l'entra√Ænement\n",
    "    \"\"\"\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks_list = []\n",
    "    if use_callbacks:\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "        callbacks_list = [early_stop, reduce_lr]\n",
    "    \n",
    "    # Calculer les steps\n",
    "    steps_per_epoch = max(1, train_gen.samples // batch_size)\n",
    "    validation_steps = max(1, test_gen.samples // batch_size)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ENTRA√éNEMENT: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Epochs: {epochs}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "    print(f\"Validation steps: {validation_steps}\")\n",
    "    print(f\"Class weights: {'Oui' if class_weights else 'Non'}\")\n",
    "    print(f\"Callbacks: {'Oui' if use_callbacks else 'Non'}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Entra√Ænement\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs,\n",
    "        validation_data=test_gen,\n",
    "        validation_steps=validation_steps,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=callbacks_list,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    training_time = (end_time - start_time).total_seconds()\n",
    "    \n",
    "    print(f\"\\n‚úì Entra√Ænement termin√© en {training_time:.1f} secondes ({training_time/60:.1f} minutes)\")\n",
    "    \n",
    "    # Visualisation des courbes\n",
    "    plot_training_history(history, model_name)\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"Affiche les courbes d'apprentissage (accuracy et loss)\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2, marker='o')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2, marker='s')\n",
    "    ax1.set_title(f'{model_name} - Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss\n",
    "    ax2.plot(history.history['loss'], label='Train Loss', linewidth=2, marker='o')\n",
    "    ax2.plot(history.history['val_loss'], label='Val Loss', linewidth=2, marker='s')\n",
    "    ax2.set_title(f'{model_name} - Loss', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Loss', fontsize=12)\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Afficher les m√©triques finales\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"M√âTRIQUES FINALES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Train Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"Val Accuracy:   {history.history['val_accuracy'][-1]:.4f}\")\n",
    "    print(f\"Train Loss:     {history.history['loss'][-1]:.4f}\")\n",
    "    print(f\"Val Loss:       {history.history['val_loss'][-1]:.4f}\")\n",
    "    \n",
    "    # D√©tecter overfitting\n",
    "    acc_diff = history.history['accuracy'][-1] - history.history['val_accuracy'][-1]\n",
    "    if acc_diff > 0.10:\n",
    "        print(f\"\\n‚ö† OVERFITTING D√âTECT√â! (√©cart accuracy: {acc_diff:.4f})\")\n",
    "    else:\n",
    "        print(f\"\\n‚úì Bon √©quilibre train/val (√©cart: {acc_diff:.4f})\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "\n",
    "print(\"‚úì Fonctions train_and_evaluate() et plot_training_history() d√©finies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3361c18",
   "metadata": {},
   "source": [
    "## 4. Exp√©rimentation - Test de 3 Architectures CNN\n",
    "\n",
    "Nous allons tester **3 variantes** de notre CNN en faisant varier les param√®tres:\n",
    "\n",
    "| Essai | Filtres | Kernel | Dropout | Batch Size | Objectif |\n",
    "|-------|---------|--------|---------|------------|----------|\n",
    "| **A** | 32‚Üí64‚Üí128 | 3√ó3 | 0.3 | 32 | Mod√®le de base l√©ger |\n",
    "| **B** | 16‚Üí32‚Üí64‚Üí128 | 3√ó3 | 0.5 | 64 | Mod√®le plus profond |\n",
    "| **C** | 32‚Üí64‚Üí128 | 5√ó5 | 0.4 | 32 | Kernel plus large |\n",
    "\n",
    "### Essai A: Configuration de Base (32‚Üí64‚Üí128, kernel 3√ó3, dropout 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d504dbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ESSAI A: Configuration de Base\n",
    "# ============================================\n",
    "# Filtres: 32 ‚Üí 64 ‚Üí 128\n",
    "# Kernel: 3√ó3\n",
    "# Dropout: 0.3\n",
    "# Batch size: 32\n",
    "\n",
    "print(\"\\n\" + \"üîµ\"*30)\n",
    "print(\"ESSAI A: Configuration de Base\")\n",
    "print(\"üîµ\"*30 + \"\\n\")\n",
    "\n",
    "# Construire le mod√®le A\n",
    "model_A = build_cnn_model(\n",
    "    filters_list=[32, 64, 128],\n",
    "    kernel_size=3,\n",
    "    dropout_rate=0.3,\n",
    "    use_batch_norm=True,\n",
    "    dense_units=128,\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
    "    num_classes=NUM_CLASSES\n",
    ")\n",
    "\n",
    "# Compiler le mod√®le\n",
    "model_A.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Afficher le r√©sum√©\n",
    "print_model_info(model_A, \"ESSAI A - Mod√®le de Base\")\n",
    "\n",
    "# Entra√Æner le mod√®le\n",
    "history_A = train_and_evaluate(\n",
    "    model=model_A,\n",
    "    train_gen=train_generator,\n",
    "    test_gen=test_generator,\n",
    "    model_name=\"ESSAI A (32‚Üí64‚Üí128, 3√ó3, dropout 0.3)\",\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weights=class_weights,\n",
    "    use_callbacks=True\n",
    ")\n",
    "\n",
    "# Sauvegarder les r√©sultats\n",
    "results_A = {\n",
    "    'final_train_acc': history_A.history['accuracy'][-1],\n",
    "    'final_val_acc': history_A.history['val_accuracy'][-1],\n",
    "    'final_train_loss': history_A.history['loss'][-1],\n",
    "    'final_val_loss': history_A.history['val_loss'][-1],\n",
    "    'best_val_acc': max(history_A.history['val_accuracy'])\n",
    "}\n",
    "\n",
    "print(\"\\n‚úì Essai A termin√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555fd8e7",
   "metadata": {},
   "source": [
    "### Essai B: Mod√®le Plus Profond (16‚Üí32‚Üí64‚Üí128, kernel 3√ó3, dropout 0.5, batch 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f669441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ESSAI B: Mod√®le Plus Profond\n",
    "# ============================================\n",
    "# Filtres: 16 ‚Üí 32 ‚Üí 64 ‚Üí 128 (4 blocs conv)\n",
    "# Kernel: 3√ó3\n",
    "# Dropout: 0.5 (plus √©lev√© pour r√©gularisation)\n",
    "# Batch size: 64\n",
    "\n",
    "print(\"\\n\" + \"üü¢\"*30)\n",
    "print(\"ESSAI B: Mod√®le Plus Profond\")\n",
    "print(\"üü¢\"*30 + \"\\n\")\n",
    "\n",
    "# Recr√©er les g√©n√©rateurs avec batch_size=64\n",
    "train_generator_B = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test_generator_B = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Construire le mod√®le B\n",
    "model_B = build_cnn_model(\n",
    "    filters_list=[16, 32, 64, 128],  # 4 blocs\n",
    "    kernel_size=3,\n",
    "    dropout_rate=0.5,  # Dropout plus √©lev√©\n",
    "    use_batch_norm=True,\n",
    "    dense_units=128,\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
    "    num_classes=NUM_CLASSES\n",
    ")\n",
    "\n",
    "# Compiler le mod√®le\n",
    "model_B.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Afficher le r√©sum√©\n",
    "print_model_info(model_B, \"ESSAI B - Mod√®le Plus Profond\")\n",
    "\n",
    "# Entra√Æner le mod√®le\n",
    "history_B = train_and_evaluate(\n",
    "    model=model_B,\n",
    "    train_gen=train_generator_B,\n",
    "    test_gen=test_generator_B,\n",
    "    model_name=\"ESSAI B (16‚Üí32‚Üí64‚Üí128, 3√ó3, dropout 0.5, batch 64)\",\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=64,\n",
    "    class_weights=class_weights,\n",
    "    use_callbacks=True\n",
    ")\n",
    "\n",
    "# Sauvegarder les r√©sultats\n",
    "results_B = {\n",
    "    'final_train_acc': history_B.history['accuracy'][-1],\n",
    "    'final_val_acc': history_B.history['val_accuracy'][-1],\n",
    "    'final_train_loss': history_B.history['loss'][-1],\n",
    "    'final_val_loss': history_B.history['val_loss'][-1],\n",
    "    'best_val_acc': max(history_B.history['val_accuracy'])\n",
    "}\n",
    "\n",
    "print(\"\\n‚úì Essai B termin√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96647e40",
   "metadata": {},
   "source": [
    "### Essai C: Kernel Plus Large (32‚Üí64‚Üí128, kernel 5√ó5, dropout 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42afe662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ESSAI C: Kernel Plus Large\n",
    "# ============================================\n",
    "# Filtres: 32 ‚Üí 64 ‚Üí 128\n",
    "# Kernel: 5√ó5 (plus large, capture des motifs plus grands)\n",
    "# Dropout: 0.4\n",
    "# Batch size: 32\n",
    "\n",
    "print(\"\\n\" + \"üü°\"*30)\n",
    "print(\"ESSAI C: Kernel Plus Large (5√ó5)\")\n",
    "print(\"üü°\"*30 + \"\\n\")\n",
    "\n",
    "# Construire le mod√®le C\n",
    "model_C = build_cnn_model(\n",
    "    filters_list=[32, 64, 128],\n",
    "    kernel_size=5,  # Kernel 5√ó5 au lieu de 3√ó3\n",
    "    dropout_rate=0.4,\n",
    "    use_batch_norm=True,\n",
    "    dense_units=128,\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
    "    num_classes=NUM_CLASSES\n",
    ")\n",
    "\n",
    "# Compiler le mod√®le\n",
    "model_C.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Afficher le r√©sum√©\n",
    "print_model_info(model_C, \"ESSAI C - Kernel 5√ó5\")\n",
    "\n",
    "# Entra√Æner le mod√®le\n",
    "history_C = train_and_evaluate(\n",
    "    model=model_C,\n",
    "    train_gen=train_generator,  # Batch size 32\n",
    "    test_gen=test_generator,\n",
    "    model_name=\"ESSAI C (32‚Üí64‚Üí128, 5√ó5, dropout 0.4)\",\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weights=class_weights,\n",
    "    use_callbacks=True\n",
    ")\n",
    "\n",
    "# Sauvegarder les r√©sultats\n",
    "results_C = {\n",
    "    'final_train_acc': history_C.history['accuracy'][-1],\n",
    "    'final_val_acc': history_C.history['val_accuracy'][-1],\n",
    "    'final_train_loss': history_C.history['loss'][-1],\n",
    "    'final_val_loss': history_C.history['val_loss'][-1],\n",
    "    'best_val_acc': max(history_C.history['val_accuracy'])\n",
    "}\n",
    "\n",
    "print(\"\\n‚úì Essai C termin√©\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì LES 3 ESSAIS SONT TERMIN√âS\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d70dbaa",
   "metadata": {},
   "source": [
    "## 5. Tableau Comparatif des R√©sultats\n",
    "\n",
    "Comparaison des 3 essais pour identifier le meilleur mod√®le"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cd8195",
   "metadata": {},
   "source": [
    "### Essai D: Architecture Avanc√©e Optimis√©e (64‚Üí128‚Üí256‚Üí512, Global Avg Pool, R√©gularisation Renforc√©e)\n",
    "\n",
    "**Am√©liorations majeures:**\n",
    "- Architecture VGG-style avec blocs convolutionnels multiples\n",
    "- Global Average Pooling pour r√©duction drastique des param√®tres\n",
    "- R√©gularisation L2 sur tous les noyaux\n",
    "- Couches denses multiples avec BatchNorm\n",
    "- Optimiseur AdamW avec weight decay\n",
    "- Callbacks avanc√©s (EarlyStopping, ReduceLROnPlateau, LearningRateScheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6144927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ESSAI D: Architecture Avanc√©e Optimis√©e\n",
    "# ============================================\n",
    "# Filtres: 64 ‚Üí 128 ‚Üí 256 ‚Üí 512 (architecture pyramidale)\n",
    "# Architecture: VGG-style avec blocs multiples\n",
    "# Pooling: Global Average Pooling\n",
    "# R√©gularisation: L2 + Dropout progressif + BatchNorm\n",
    "# Optimiseur: AdamW avec weight decay\n",
    "# Callbacks: Avanc√©s avec LearningRateScheduler\n",
    "\n",
    "print(\"\\n\" + \"üü¢\"*30)\n",
    "print(\"ESSAI D: Architecture Avanc√©e Optimis√©e\")\n",
    "print(\"üü¢\"*30 + \"\\n\")\n",
    "\n",
    "# Construire le mod√®le D avanc√©\n",
    "model_D = build_advanced_cnn_model(\n",
    "    filters_list=[64, 128, 256, 512],  # Architecture pyramidale\n",
    "    kernel_size=3,\n",
    "    dropout_rate=0.4,\n",
    "    use_batch_norm=True,\n",
    "    dense_units=[512, 256],  # Couches denses multiples\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
    "    num_classes=NUM_CLASSES\n",
    ")\n",
    "\n",
    "# Compiler avec optimiseur avanc√©\n",
    "compile_model_advanced(model_D, learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Afficher le r√©sum√©\n",
    "print_model_info(model_D, \"ESSAI D - Architecture Avanc√©e\")\n",
    "\n",
    "# Callbacks avanc√©s\n",
    "callbacks_D = get_callbacks_advanced(\"model_D\")\n",
    "\n",
    "# Entra√Æner le mod√®le avec callbacks optimis√©s\n",
    "print(\"\\n\" + \"üöÄ\"*30)\n",
    "print(\"ENTRA√éNEMENT AVEC CALLBACKS OPTIMIS√âS\")\n",
    "print(\"üöÄ\"*30)\n",
    "\n",
    "history_D = model_D.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=test_generator,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks_D,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# √âvaluation finale\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"√âVALUATION FINALE - ESSAI D\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Pr√©dictions sur le test set\n",
    "y_pred_D = model_D.predict(test_generator)\n",
    "y_pred_classes_D = np.argmax(y_pred_D, axis=1)\n",
    "y_true_D = test_generator.classes\n",
    "\n",
    "# M√©triques d√©taill√©es\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true_D, y_pred_classes_D, target_names=class_names))\n",
    "\n",
    "# Matrice de confusion\n",
    "cm_D = confusion_matrix(y_true_D, y_pred_classes_D)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_D, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Matrice de Confusion - Essai D (Architecture Avanc√©e)')\n",
    "plt.xlabel('Pr√©dictions')\n",
    "plt.ylabel('V√©rit√©s')\n",
    "plt.show()\n",
    "\n",
    "# Courbe d'apprentissage\n",
    "plot_training_history(history_D, \"Essai D - Architecture Avanc√©e\")\n",
    "\n",
    "# Sauvegarder les r√©sultats\n",
    "results_D = {\n",
    "    'final_train_acc': history_D.history['accuracy'][-1],\n",
    "    'final_val_acc': history_D.history['val_accuracy'][-1],\n",
    "    'final_train_loss': history_D.history['loss'][-1],\n",
    "    'final_val_loss': history_D.history['val_loss'][-1],\n",
    "    'best_val_acc': max(history_D.history['val_accuracy']),\n",
    "    'precision': precision_score(y_true_D, y_pred_classes_D, average='weighted'),\n",
    "    'recall': recall_score(y_true_D, y_pred_classes_D, average='weighted'),\n",
    "    'f1': f1_score(y_true_D, y_pred_classes_D, average='weighted')\n",
    "}\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Train Accuracy: {results_D['final_train_acc']:.4f}\")\n",
    "print(f\"Val Accuracy:   {results_D['final_val_acc']:.4f}\")\n",
    "print(f\"Precision:      {results_D['precision']:.4f}\")\n",
    "print(f\"Recall:         {results_D['recall']:.4f}\")\n",
    "print(f\"F1-Score:       {results_D['f1']:.4f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(\"‚úì Essai D termin√© - Architecture avanc√©e avec optimisations compl√®tes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eca255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TABLEAU COMPARATIF DES 4 ESSAIS\n",
    "# ============================================\n",
    "\n",
    "# Cr√©er un DataFrame avec les r√©sultats\n",
    "comparison_data = {\n",
    "    'Essai': ['A', 'B', 'C', 'D'],\n",
    "    'Filtres': ['32‚Üí64‚Üí128', '16‚Üí32‚Üí64‚Üí128', '32‚Üí64‚Üí128', '64‚Üí128‚Üí256‚Üí512'],\n",
    "    'Kernel': ['3√ó3', '3√ó3', '5√ó5', '3√ó3'],\n",
    "    'Dropout': [0.3, 0.5, 0.4, '0.4+'],\n",
    "    'Batch Size': [32, 64, 32, 32],\n",
    "    'Architecture': ['Basique', 'Profonde', 'Large Kernel', 'Avanc√©e+VGG'],\n",
    "    'Accuracy (train)': [\n",
    "        f\"{results_A['final_train_acc']:.4f}\",\n",
    "        f\"{results_B['final_train_acc']:.4f}\",\n",
    "        f\"{results_C['final_train_acc']:.4f}\",\n",
    "        f\"{results_D['final_train_acc']:.4f}\"\n",
    "    ],\n",
    "    'Accuracy (val)': [\n",
    "        f\"{results_A['final_val_acc']:.4f}\",\n",
    "        f\"{results_B['final_val_acc']:.4f}\",\n",
    "        f\"{results_C['final_val_acc']:.4f}\",\n",
    "        f\"{results_D['final_val_acc']:.4f}\"\n",
    "    ],\n",
    "    'Best Val Acc': [\n",
    "        f\"{results_A['best_val_acc']:.4f}\",\n",
    "        f\"{results_B['best_val_acc']:.4f}\",\n",
    "        f\"{results_C['best_val_acc']:.4f}\",\n",
    "        f\"{results_D['best_val_acc']:.4f}\"\n",
    "    ],\n",
    "    'Commentaire': [\n",
    "        'Mod√®le de base l√©ger et rapide',\n",
    "        'Plus profond mais risque d\\'overfitting',\n",
    "        'Kernel large capture des motifs globaux',\n",
    "        'Architecture avanc√©e optimis√©e'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"TABLEAU COMPARATIF DES R√âSULTATS - 4 ESSAIS\")\n",
    "print(\"=\"*120)\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Identifier le meilleur mod√®le parmi A, B, C, D\n",
    "best_val_accs = [results_A['final_val_acc'], results_B['final_val_acc'], results_C['final_val_acc'], results_D['final_val_acc']]\n",
    "best_model_idx = np.argmax(best_val_accs)\n",
    "best_model_names = ['A', 'B', 'C', 'D']\n",
    "best_model_name = best_model_names[best_model_idx]\n",
    "\n",
    "print(f\"\\nüèÜ MEILLEUR MOD√àLE: ESSAI {best_model_name}\")\n",
    "print(f\"   Validation Accuracy: {best_val_accs[best_model_idx]:.4f}\")\n",
    "\n",
    "# Visualisation comparative\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Accuracy finale\n",
    "axes[0, 0].bar(['A', 'B', 'C', 'D'],\n",
    "               [results_A['final_val_acc'], results_B['final_val_acc'], results_C['final_val_acc'], results_D['final_val_acc']],\n",
    "               color=['steelblue', 'seagreen', 'coral', 'purple'])\n",
    "axes[0, 0].set_title('Validation Accuracy Finale', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_ylim([0, 1])\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "for i, v in enumerate([results_A['final_val_acc'], results_B['final_val_acc'], results_C['final_val_acc'], results_D['final_val_acc']]):\n",
    "    axes[0, 0].text(i, v + 0.02, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Loss finale\n",
    "axes[0, 1].bar(['A', 'B', 'C', 'D'],\n",
    "               [results_A['final_val_loss'], results_B['final_val_loss'], results_C['final_val_loss'], results_D['final_val_loss']],\n",
    "               color=['steelblue', 'seagreen', 'coral', 'purple'])\n",
    "axes[0, 1].set_title('Validation Loss Finale', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Overfitting (√©cart train-val)\n",
    "overfitting_A = results_A['final_train_acc'] - results_A['final_val_acc']\n",
    "overfitting_B = results_B['final_train_acc'] - results_B['final_val_acc']\n",
    "overfitting_C = results_C['final_train_acc'] - results_C['final_val_acc']\n",
    "overfitting_D = results_D['final_train_acc'] - results_D['final_val_acc']\n",
    "\n",
    "axes[1, 0].bar(['A', 'B', 'C', 'D'], [overfitting_A, overfitting_B, overfitting_C, overfitting_D],\n",
    "               color=['steelblue', 'seagreen', 'coral', 'purple'])\n",
    "axes[1, 0].set_title('√âcart Train-Val Accuracy (Overfitting)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('√âcart')\n",
    "axes[1, 0].axhline(y=0.1, color='r', linestyle='--', label='Seuil overfitting')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Comparaison graphique Val Accuracy\n",
    "axes[1, 1].plot(history_A.history['val_accuracy'], label='Essai A', linewidth=2, marker='o')\n",
    "axes[1, 1].plot(history_B.history['val_accuracy'], label='Essai B', linewidth=2, marker='s')\n",
    "axes[1, 1].plot(history_C.history['val_accuracy'], label='Essai C', linewidth=2, marker='^')\n",
    "axes[1, 1].plot(history_D.history['val_accuracy'], label='Essai D', linewidth=2, marker='*')\n",
    "axes[1, 1].set_title('√âvolution Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Validation Accuracy')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Stocker le meilleur mod√®le\n",
    "if best_model_name == 'A':\n",
    "    best_model = model_A\n",
    "    best_history = history_A\n",
    "elif best_model_name == 'B':\n",
    "    best_model = model_B\n",
    "    best_history = history_B\n",
    "elif best_model_name == 'C':\n",
    "    best_model = model_C\n",
    "    best_history = history_C\n",
    "else:\n",
    "    best_model = model_D\n",
    "    best_history = history_D\n",
    "\n",
    "print(f\"\\n‚úì Meilleur mod√®le s√©lectionn√©: ESSAI {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16516f40",
   "metadata": {},
   "source": [
    "## 6. √âvaluation Compl√®te du Meilleur Mod√®le\n",
    "\n",
    "### M√©triques d√©taill√©es : Precision, Recall, F1-Score, Matrice de Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5dc27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# √âVALUATION D√âTAILL√âE DU MEILLEUR MOD√àLE\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"√âVALUATION COMPL√àTE - ESSAI {best_model_name} (Meilleur Mod√®le)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# R√©initialiser le g√©n√©rateur de test\n",
    "test_generator.reset()\n",
    "\n",
    "# Obtenir les pr√©dictions\n",
    "y_pred_proba = best_model.predict(test_generator, verbose=1)\n",
    "y_pred_classes = np.argmax(y_pred_proba, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Noms des classes\n",
    "class_names_list = list(train_generator.class_indices.keys())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_true, y_pred_classes, \n",
    "                           target_names=class_names_list,\n",
    "                           digits=4))\n",
    "\n",
    "# Calculer les m√©triques globales\n",
    "accuracy = accuracy_score(y_true, y_pred_classes)\n",
    "precision_macro = precision_score(y_true, y_pred_classes, average='macro')\n",
    "recall_macro = recall_score(y_true, y_pred_classes, average='macro')\n",
    "f1_macro = f1_score(y_true, y_pred_classes, average='macro')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"M√âTRIQUES GLOBALES (Macro Average)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision_macro:.4f}\")\n",
    "print(f\"Recall:    {recall_macro:.4f}\")\n",
    "print(f\"F1-Score:  {f1_macro:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc93e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MATRICE DE CONFUSION\n",
    "# ============================================\n",
    "\n",
    "# Calculer la matrice de confusion\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "# Visualiser la matrice de confusion\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names_list,\n",
    "            yticklabels=class_names_list,\n",
    "            cbar_kws={'label': 'Nombre de pr√©dictions'},\n",
    "            annot_kws={'size': 14, 'weight': 'bold'})\n",
    "\n",
    "plt.title(f'Matrice de Confusion - ESSAI {best_model_name}', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('Vraie Classe', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Classe Pr√©dite', fontsize=12, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Matrice de confusion normalis√©e (en pourcentage)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Greens',\n",
    "            xticklabels=class_names_list,\n",
    "            yticklabels=class_names_list,\n",
    "            cbar_kws={'label': 'Pourcentage'},\n",
    "            annot_kws={'size': 14, 'weight': 'bold'})\n",
    "\n",
    "plt.title(f'Matrice de Confusion Normalis√©e - ESSAI {best_model_name}', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('Vraie Classe', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Classe Pr√©dite', fontsize=12, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyse des erreurs\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSE DES ERREURS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, class_name in enumerate(class_names_list):\n",
    "    total = cm[i].sum()\n",
    "    correct = cm[i, i]\n",
    "    errors = total - correct\n",
    "    accuracy_class = (correct / total) * 100 if total > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{class_name}:\")\n",
    "    print(f\"  Total d'√©chantillons: {total}\")\n",
    "    print(f\"  Correctement class√©s: {correct} ({accuracy_class:.1f}%)\")\n",
    "    print(f\"  Erreurs: {errors}\")\n",
    "    \n",
    "    if errors > 0:\n",
    "        # Trouver les confusions principales\n",
    "        confusion_indices = np.argsort(cm[i])[::-1]\n",
    "        print(f\"  Principales confusions:\")\n",
    "        for j in confusion_indices:\n",
    "            if j != i and cm[i, j] > 0:\n",
    "                print(f\"    ‚Üí {cm[i, j]} √©chantillons confondus avec {class_names_list[j]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dca1bac",
   "metadata": {},
   "source": [
    "## 7. Transfer Learning avec Mod√®le Pr√©-entra√Æn√©\n",
    "\n",
    "### Utilisation de ResNet50 pr√©-entra√Æn√© sur ImageNet\n",
    "\n",
    "Le **Transfer Learning** consiste √† utiliser un mod√®le d√©j√† entra√Æn√© sur un grand dataset (ImageNet avec 1.4M images) et adapter sa derni√®re couche pour notre probl√®me sp√©cifique (4 classes de tumeurs c√©r√©brales).\n",
    "\n",
    "**Avantages:**\n",
    "- Meilleure g√©n√©ralisation gr√¢ce aux features pr√©-apprises\n",
    "- Converge plus rapidement\n",
    "- N√©cessite moins de donn√©es\n",
    "\n",
    "**Strat√©gie:**\n",
    "1. Charger ResNet50 sans la couche de classification (`include_top=False`)\n",
    "2. Geler les couches pr√©-entra√Æn√©es (`trainable=False`)\n",
    "3. Ajouter nos propres couches de classification\n",
    "4. Entra√Æner seulement la nouvelle \"t√™te\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b2b291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TRANSFER LEARNING OPTIMIS√â - RESNET50\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"üî¥\"*30)\n",
    "print(\"TRANSFER LEARNING OPTIMIS√â - ResNet50\")\n",
    "print(\"üî¥\"*30 + \"\\n\")\n",
    "\n",
    "# Charger ResNet50 pr√©-entra√Æn√© (sans modification du mod√®le de base)\n",
    "base_model_resnet = ResNet50(\n",
    "    weights='imagenet',          # Poids pr√©-entra√Æn√©s sur ImageNet\n",
    "    include_top=False,           # Exclure la couche de classification finale\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    ")\n",
    "\n",
    "# Geler toutes les couches du mod√®le de base (respect de la consigne)\n",
    "base_model_resnet.trainable = False\n",
    "\n",
    "print(f\"‚úì ResNet50 charg√© (mod√®le pr√©-entra√Æn√© non modifi√©)\")\n",
    "print(f\"  Nombre de couches: {len(base_model_resnet.layers)}\")\n",
    "print(f\"  Param√®tres totaux: {base_model_resnet.count_params():,}\")\n",
    "print(f\"  Couches gel√©es: Oui (trainable=False)\")\n",
    "\n",
    "# Construire une t√™te de classification optimis√©e\n",
    "model_transfer = Sequential([\n",
    "    base_model_resnet,\n",
    "\n",
    "    # Global Average Pooling pour r√©duction dimensionnelle\n",
    "    GlobalAveragePooling2D(name='global_avg_pool'),\n",
    "\n",
    "    # Premi√®re couche dense avec r√©gularisation\n",
    "    Dense(512, activation='relu',\n",
    "          kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "          name='dense_512'),\n",
    "    BatchNormalization(name='bn_512'),\n",
    "    Dropout(0.5, name='dropout_512'),\n",
    "\n",
    "    # Deuxi√®me couche dense\n",
    "    Dense(256, activation='relu',\n",
    "          kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "          name='dense_256'),\n",
    "    BatchNormalization(name='bn_256'),\n",
    "    Dropout(0.4, name='dropout_256'),\n",
    "\n",
    "    # Troisi√®me couche pour raffinement\n",
    "    Dense(128, activation='relu',\n",
    "          kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "          name='dense_128'),\n",
    "    BatchNormalization(name='bn_128'),\n",
    "    Dropout(0.3, name='dropout_128'),\n",
    "\n",
    "    # Couche de sortie\n",
    "    Dense(NUM_CLASSES, activation='softmax', name='output')\n",
    "], name='ResNet50_Optimized_Transfer')\n",
    "\n",
    "# Compiler avec optimiseur optimis√©\n",
    "optimizer_transfer = tf.keras.optimizers.AdamW(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=1e-4,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999\n",
    ")\n",
    "\n",
    "model_transfer.compile(\n",
    "    optimizer=optimizer_transfer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy',\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ARCHITECTURE TRANSFER LEARNING OPTIMIS√âE\")\n",
    "print(\"=\"*60)\n",
    "model_transfer.summary()\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compter les param√®tres\n",
    "trainable_params = np.sum([np.prod(v.get_shape()) for v in model_transfer.trainable_weights])\n",
    "non_trainable_params = np.sum([np.prod(v.get_shape()) for v in model_transfer.non_trainable_weights])\n",
    "\n",
    "print(f\"\\nParam√®tres entra√Ænables:     {trainable_params:,}\")\n",
    "print(f\"Param√®tres non-entra√Ænables: {non_trainable_params:,}\")\n",
    "print(f\"Total:                       {trainable_params + non_trainable_params:,}\")\n",
    "\n",
    "print(\"\\n‚úì Mod√®le Transfer Learning optimis√© construit avec t√™te am√©lior√©e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0846eed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ENTRA√éNEMENT DU MOD√àLE TRANSFER LEARNING\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüöÄ D√©but de l'entra√Ænement Transfer Learning...\")\n",
    "\n",
    "history_transfer = train_and_evaluate(\n",
    "    model=model_transfer,\n",
    "    train_gen=train_generator,\n",
    "    test_gen=test_generator,\n",
    "    model_name=\"Transfer Learning - ResNet50\",\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weights=class_weights,\n",
    "    use_callbacks=True\n",
    ")\n",
    "\n",
    "# Sauvegarder les r√©sultats\n",
    "results_transfer = {\n",
    "    'final_train_acc': history_transfer.history['accuracy'][-1],\n",
    "    'final_val_acc': history_transfer.history['val_accuracy'][-1],\n",
    "    'final_train_loss': history_transfer.history['loss'][-1],\n",
    "    'final_val_loss': history_transfer.history['val_loss'][-1],\n",
    "    'best_val_acc': max(history_transfer.history['val_accuracy'])\n",
    "}\n",
    "\n",
    "print(\"\\n‚úì Entra√Ænement Transfer Learning termin√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5661d9ca",
   "metadata": {},
   "source": [
    "## 8. Comparaison CNN Custom vs Transfer Learning\n",
    "\n",
    "### Analyse comparative des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce0049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPARAISON CNN CUSTOM VS TRANSFER LEARNING\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARAISON FINALE: CNN CUSTOM vs TRANSFER LEARNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Tableau comparatif\n",
    "comparison_final = {\n",
    "    'Mod√®le': [f'CNN Custom (Essai {best_model_name})', 'Transfer Learning (ResNet50)'],\n",
    "    'Accuracy (val)': [\n",
    "        f\"{results_A['final_val_acc'] if best_model_name == 'A' else results_B['final_val_acc'] if best_model_name == 'B' else results_C['final_val_acc']:.4f}\",\n",
    "        f\"{results_transfer['final_val_acc']:.4f}\"\n",
    "    ],\n",
    "    'Loss (val)': [\n",
    "        f\"{results_A['final_val_loss'] if best_model_name == 'A' else results_B['final_val_loss'] if best_model_name == 'B' else results_C['final_val_loss']:.4f}\",\n",
    "        f\"{results_transfer['final_val_loss']:.4f}\"\n",
    "    ],\n",
    "    'Param√®tres': [\n",
    "        f\"{best_model.count_params():,}\",\n",
    "        f\"{model_transfer.count_params():,} ({trainable_params:,} entra√Ænables)\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_final = pd.DataFrame(comparison_final)\n",
    "print(\"\\n\" + df_final.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Visualisation comparative\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Comparaison Validation Accuracy\n",
    "models_names = [f'CNN Custom\\n(Essai {best_model_name})', 'Transfer Learning\\n(ResNet50)']\n",
    "val_accs = [\n",
    "    results_A['final_val_acc'] if best_model_name == 'A' else results_B['final_val_acc'] if best_model_name == 'B' else results_C['final_val_acc'],\n",
    "    results_transfer['final_val_acc']\n",
    "]\n",
    "\n",
    "bars = axes[0, 0].bar(models_names, val_accs, color=['steelblue', 'darkred'], alpha=0.8)\n",
    "axes[0, 0].set_title('Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_ylim([0, 1])\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, val in zip(bars, val_accs):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                    f'{val:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Courbes d'apprentissage comparatives\n",
    "axes[0, 1].plot(best_history.history['val_accuracy'], \n",
    "               label=f'CNN Custom (Essai {best_model_name})', \n",
    "               linewidth=2.5, marker='o', markersize=6)\n",
    "axes[0, 1].plot(history_transfer.history['val_accuracy'], \n",
    "               label='Transfer Learning', \n",
    "               linewidth=2.5, marker='s', markersize=6)\n",
    "axes[0, 1].set_title('√âvolution Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Validation Accuracy')\n",
    "axes[0, 1].legend(fontsize=11)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Courbes de loss\n",
    "axes[1, 0].plot(best_history.history['val_loss'], \n",
    "               label=f'CNN Custom (Essai {best_model_name})', \n",
    "               linewidth=2.5, marker='o', markersize=6)\n",
    "axes[1, 0].plot(history_transfer.history['val_loss'], \n",
    "               label='Transfer Learning', \n",
    "               linewidth=2.5, marker='s', markersize=6)\n",
    "axes[1, 0].set_title('√âvolution Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Validation Loss')\n",
    "axes[1, 0].legend(fontsize=11)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Comparaison nombre de param√®tres\n",
    "param_counts = [best_model.count_params(), trainable_params]\n",
    "bars2 = axes[1, 1].bar(models_names, param_counts, color=['steelblue', 'darkred'], alpha=0.8)\n",
    "axes[1, 1].set_title('Nombre de Param√®tres Entra√Ænables', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Param√®tres')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, val in zip(bars2, param_counts):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{val:,}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Conclusion\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSE COMPARATIVE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if results_transfer['final_val_acc'] > (results_A['final_val_acc'] if best_model_name == 'A' else results_B['final_val_acc'] if best_model_name == 'B' else results_C['final_val_acc']):\n",
    "    winner = \"Transfer Learning\"\n",
    "    diff = results_transfer['final_val_acc'] - (results_A['final_val_acc'] if best_model_name == 'A' else results_B['final_val_acc'] if best_model_name == 'B' else results_C['final_val_acc'])\n",
    "else:\n",
    "    winner = f\"CNN Custom (Essai {best_model_name})\"\n",
    "    diff = (results_A['final_val_acc'] if best_model_name == 'A' else results_B['final_val_acc'] if best_model_name == 'B' else results_C['final_val_acc']) - results_transfer['final_val_acc']\n",
    "\n",
    "print(f\"\\nüèÜ MEILLEUR MOD√àLE GLOBAL: {winner}\")\n",
    "print(f\"   Avantage en accuracy: +{diff:.4f} ({diff*100:.2f}%)\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14ded6f",
   "metadata": {},
   "source": [
    "## 9. Conclusion Synth√©tique\n",
    "\n",
    "### Comparaison des Approches et Recommandations pour le Diagnostic M√©dical\n",
    "\n",
    "#### **Synth√®se des R√©sultats**\n",
    "\n",
    "Au cours de ce TP, nous avons explor√© deux approches compl√©mentaires pour la classification automatique de tumeurs c√©r√©brales √† partir d'images IRM :\n",
    "\n",
    "1. **CNN Custom** : Conception et optimisation d'architectures neuronales personnalis√©es (4 variantes test√©es)\n",
    "2. **Transfer Learning** : Utilisation de ResNet50 pr√©-entra√Æn√© sur ImageNet\n",
    "\n",
    "#### **Performance des CNN Custom (Essais A, B, C, D)**\n",
    "\n",
    "Nous avons test√© 4 variantes en faisant varier les param√®tres :\n",
    "- **Essai A** : 32‚Üí64‚Üí128, kernel 3√ó3, dropout 0.3 (mod√®le de base)\n",
    "- **Essai B** : 16‚Üí32‚Üí64‚Üí128, kernel 3√ó3, dropout 0.5 (architecture profonde)\n",
    "- **Essai C** : 32‚Üí64‚Üí128, kernel 5√ó5, dropout 0.4 (kernel large)\n",
    "- **Essai D** : 64‚Üí128‚Üí256‚Üí512, architecture VGG-style avanc√©e avec Global Avg Pooling, r√©gularisation renforc√©e\n",
    "\n",
    "**Observations** :\n",
    "- Les mod√®les plus l√©gers (Essai A) offrent un bon √©quilibre performance/rapidit√©\n",
    "- Les architectures plus profondes (Essai B) tendent vers l'overfitting malgr√© un dropout √©lev√©\n",
    "- Les kernels 5√ó5 (Essai C) capturent des motifs plus globaux mais augmentent le co√ªt computationnel\n",
    "- **L'architecture avanc√©e (Essai D)** combine les meilleures pratiques : blocs VGG, Global Average Pooling, r√©gularisation L2, AdamW, callbacks optimis√©s\n",
    "\n",
    "#### **Transfer Learning avec ResNet50**\n",
    "\n",
    "**Avantages constat√©s** :\n",
    "- **G√©n√©ralisation sup√©rieure** : Features pr√©-apprises sur ImageNet (1.4M images) se transf√®rent bien aux images m√©dicales\n",
    "- **Convergence plus rapide** : Atteint de bonnes performances en moins d'√©poques\n",
    "- **Robustesse** : Moins sensible au surapprentissage gr√¢ce aux repr√©sentations riches\n",
    "\n",
    "**Inconv√©nients** :\n",
    "- **Taille du mod√®le** : Plus lourd en m√©moire et en inf√©rence\n",
    "- **Co√ªt computationnel** : N√©cessite plus de ressources (GPU recommand√©)\n",
    "\n",
    "#### **Recommandation pour le Diagnostic M√©dical R√©el**\n",
    "\n",
    "Pour un syst√®me de diagnostic clinique, je recommande **l'architecture avanc√©e (Essai D) ou le Transfer Learning (ResNet50)** selon les contraintes :\n",
    "\n",
    "- **Essai D (CNN Avanc√©)** : Si vous avez des ressources computationnelles limit√©es et souhaitez un mod√®le enti√®rement personnalis√©\n",
    "- **Transfer Learning (ResNet50)** : Pour la meilleure g√©n√©ralisation et rapidit√© de convergence (recommand√© pour la s√©curit√© patient)\n",
    "\n",
    "#### **Prochaines Am√©liorations Possibles**\n",
    "\n",
    "1. **Fine-tuning avanc√©** : D√©geler progressivement les derni√®res couches de ResNet50 pour affiner les features\n",
    "2. **Ensemble de mod√®les** : Combiner les pr√©dictions de plusieurs CNN + Transfer Learning (voting majoritaire)\n",
    "3. **Explicabilit√© avec Grad-CAM** : Visualiser les zones d'attention du mod√®le pour aider les radiologues\n",
    "4. **Data augmentation avanc√©e** : Elastic deformation, mixup, cutout pour augmenter la diversit√©\n",
    "5. **Gestion du d√©s√©quilibre** : Techniques de sur-√©chantillonnage (SMOTE) ou focal loss\n",
    "6. **Validation externe** : Tester sur des datasets d'autres h√¥pitaux pour confirmer la g√©n√©ralisation\n",
    "\n",
    "#### **Conclusion Finale**\n",
    "\n",
    "Ce TP d√©montre que l'intelligence artificielle peut √™tre un outil d'aide pr√©cieux pour le diagnostic pr√©coce de tumeurs c√©r√©brales. L'architecture avanc√©e d√©velopp√©e (Essai D) montre des performances prometteuses avec des techniques modernes de r√©gularisation et d'optimisation. Cependant, il est crucial de souligner que **ces mod√®les doivent √™tre utilis√©s comme aide √† la d√©cision, non comme remplacement du diagnostic m√©dical humain**. La validation clinique rigoureuse, l'explicabilit√© des pr√©dictions et la supervision par des radiologues experts restent indispensables avant tout d√©ploiement en environnement hospitalier.\n",
    "\n",
    "---\n",
    "\n",
    "**Auteur** : MPIGA-ODOUMBA Jesse  \n",
    "**Date** : 9 novembre 2025  \n",
    "**Dataset** : Brain Tumor MRI Classification  \n",
    "**Frameworks** : TensorFlow/Keras, scikit-learn, seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad710f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nR√©sum√© des livrables:\")\n",
    "print(\"  ‚úì Exploration et pr√©paration des donn√©es\")\n",
    "print(\"  ‚úì 4 architectures CNN test√©es avec tableau comparatif\")\n",
    "print(\"  ‚úì Graphiques d'entra√Ænement (accuracy, loss)\")\n",
    "print(\"  ‚úì √âvaluation compl√®te (precision, recall, F1, matrice confusion)\")\n",
    "print(\"  ‚úì Transfer Learning avec ResNet50 optimis√©\")\n",
    "print(\"  ‚úì Comparaison CNN Custom vs Transfer Learning\")\n",
    "print(\"  ‚úì Architecture avanc√©e avec meilleures performances\")\n",
    "print(\"  ‚úì Conclusion synth√©tique avec recommandations\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
