{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8ZwIg4Wh7f26"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Flatten,Conv2D,Dropout,MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#chargement de dataset à partir de google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "print(os.listdir(\"/content/drive/MyDrive/DATASET\"))\n",
        "\n",
        "train_data=\"/content/drive/MyDrive/DATASET/TRAIN\"\n",
        "test_data=\"/content/drive/MyDrive/DATASET/TEST\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nyXDnbQDjJl",
        "outputId": "6404017b-747e-4d30-98a5-dbe51699ae38"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "['TRAIN', 'TEST']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#créer le modèle\n",
        "# On utilise un modèle séquentiel : empilement linéaire de couches\n",
        "model=Sequential()\n",
        "# === Bloc 1 : Extraction de caractéristiques simples (bords, textures)\n",
        "# - 32 filtres de taille 3x3 : détectent les motifs élémentaires.\n",
        "# - Activation ReLU : introduit la non-linéarité (met à zéro les valeurs négatives).\n",
        "# - input_shape : dimension de l’image d’entrée (ici 224x224 RGB).\n",
        "model.add(Conv2D(32,kernel_size=(3,3),activation=\"relu\",input_shape=(224,224,3)))\n",
        "# - MaxPooling2D : réduit la dimension spatiale (hauteur et largeur ÷2)\n",
        "#   tout en gardant les informations principales (valeur max).\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "# === Bloc 2 : Extraction de caractéristiques intermédiaires\n",
        "# - 64 filtres : le réseau apprend des motifs plus complexes (formes locales).\n",
        "# - Même logique de convolution + pooling.\n",
        "model.add(Conv2D(64,kernel_size=(3,3),activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "# === Bloc 3 : Extraction de caractéristiques avancées\n",
        "# - 128 filtres : capture des structures plus abstraites ou globales.\n",
        "# - Plus la profondeur augmente, plus les features deviennent conceptuelles.\n",
        "model.add(Conv2D(128,kernel_size=(3,3),activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "# === Bloc 4 : Passage de la représentation 2D à 1D\n",
        "# - Flatten : transforme les cartes de caractéristiques (H×W×C) en un vecteur 1D.\n",
        "model.add(Flatten())\n",
        "# === Bloc 5 : Couches Fully Connected (denses)\n",
        "# - 128 neurones : combinent les features extraites pour la décision finale.\n",
        "# - Activation ReLU : accélère l’apprentissage et améliore la performance.\n",
        "model.add(Dense(128,activation=\"relu\"))\n",
        "# - Dropout(0.5) : désactive aléatoirement 50 % des neurones à chaque étape\n",
        "#   pour éviter le surapprentissage (régularisation).\n",
        "model.add(Dropout(0.5))\n",
        "# === Bloc 6 : Couche de sortie\n",
        "# - 1 neurone avec activation sigmoid : adapté à une classification binaire (0 ou 1)\n",
        "#   Exemple : plastique vs métal, recyclable vs non recyclable.\n",
        "model.add(Dense(1,activation=\"sigmoid\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLNWrqxhD4vW",
        "outputId": "93c33c04-d488-4b12-a1cb-522dd564c392"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  le rôle des blocs convolutionnels du CNN\n",
        "\n",
        "Le modèle CNN est constitué de plusieurs blocs `Conv2D` et `MaxPooling2D` qui travaillent ensemble pour extraire progressivement des informations de plus en plus abstraites à partir de l’image.  \n",
        "Chaque bloc joue un rôle précis :\n",
        "\n",
        "- **Bloc 1 — 32 filtres :**  \n",
        "  Ce premier bloc agit comme une “loupe de bas niveau”.  \n",
        "  Il détecte les **motifs simples** tels que les bords, les lignes et les contrastes locaux.  \n",
        "  Ces caractéristiques servent de base à tous les motifs plus complexes des couches suivantes.\n",
        "\n",
        "- **Bloc 2 — 64 filtres :**  \n",
        "  En combinant les motifs élémentaires détectés précédemment, ce bloc apprend à reconnaître des **formes locales** et des **textures plus complexes**.  \n",
        "  Doubler le nombre de filtres permet au modèle de capturer un plus grand nombre de variations visuelles.\n",
        "\n",
        "- **Bloc 3 — 128 filtres :**  \n",
        "  À ce stade, le réseau identifie des **parties d’objets** (par exemple, une roue, un coin, un bord caractéristique d’un matériau).  \n",
        "  Les filtres deviennent plus nombreux pour modéliser la richesse des informations visuelles présentes dans les images.\n",
        "\n",
        "- **Bloc 4 — 256 filtres (facultatif selon la taille de l’image) :**  \n",
        "  Ce bloc capture une **compréhension globale** des objets présents dans l’image.  \n",
        "  Il combine plusieurs caractéristiques locales pour reconnaître des **formes complètes ou des concepts visuels entiers** (ex. : plastique, métal, verre).\n",
        "\n",
        "- **Bloc Dense final :**  \n",
        "  Les cartes de caractéristiques issues des couches convolutionnelles sont aplaties (`Flatten`) et transmises à une couche dense (`Dense`).  \n",
        "  Cette dernière combine toutes les informations extraites pour **prendre la décision finale de classification**.\n",
        "\n",
        ">  En résumé :  \n",
        "> Au fur et à mesure que l’on progresse dans les blocs (32 → 64 → 128 → 256),  \n",
        "> la taille spatiale des images diminue (à cause du `Pooling`),  \n",
        "> mais la **profondeur augmente**, ce qui permet au réseau de représenter des motifs de plus en plus abstraits et significatifs.\n"
      ],
      "metadata": {
        "id": "uKDzwvoHPvvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#compiler le modèle\n",
        "model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "Ib87o8cBFKhM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_gen=ImageDataGenerator(rescale=1./255)\n",
        "test_data_gen=ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "qewmB3VIF-o9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen=train_data_gen.flow_from_directory(\n",
        "    train_data,\n",
        "    target_size=(224,224),\n",
        "    batch_size=128,\n",
        "    color_mode=\"rgb\",\n",
        "    class_mode=\"binary\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRxyWC1TGTno",
        "outputId": "23d2fc4e-77a0-463b-b429-a6d8c23dd4d1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 22578 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_gen=test_data_gen.flow_from_directory(\n",
        "    test_data,\n",
        "    target_size=(224,224),\n",
        "    batch_size=32,\n",
        "    color_mode=\"rgb\",\n",
        "    class_mode=\"binary\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvkReAJ9JuKO",
        "outputId": "fbbb5685-0772-4a47-b814-ce26c9adbeac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2513 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(train_gen,epochs=10,validation_data=test_gen)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history[\"accuracy\"], label=\"train\")\n",
        "plt.plot(history.history[\"val_accuracy\"], label=\"val\")\n",
        "plt.title(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history[\"loss\"], label=\"train\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val\")\n",
        "plt.title(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "model.save(\"/content/drive/MyDrive/model.h5\")"
      ],
      "metadata": {
        "id": "R0ODCDDaJ6Ih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2f1ecce-aec3-4889-cb76-021e028c8a33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m  2/177\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:21:23\u001b[0m 69s/step - accuracy: 0.5234 - loss: 1.8432"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "drBS9bwvJ7Yf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}